{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training (or fine-tuning) a model\n",
    "\n",
    "The objective of this tutorial is to learn how to train a model from scratch, or fine-tune a pretrained model.  \n",
    "**Warning:** follow the [data preparation]() tutorial first and make sure the `PYANNOTE_DATABASE_CONFIG` environment variable is set accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYANNOTE_DATABASE_CONFIG'] = '/people/bredin/dev/pyannote/pyannote-db/AMI-diarization-setup/pyannote/database.yml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining which `task` the `model` will address.  \n",
    "Here, we want the `model` to address voice activity detection using the AMI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.database import get_protocol\n",
    "ami = get_protocol('AMI.SpeakerDiarization.only_words')\n",
    "\n",
    "from pyannote.audio.tasks import VoiceActivityDetection\n",
    "vad = VoiceActivityDetection(ami)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this tutorial, we define a `compute_model_fscore` function that runs a model on the first file of AMI test set and returns the voice activity detection F-score. It also displays the output of the model on the second minute of this file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio.core.inference import Inference\n",
    "from pyannote.audio.utils.signal import Binarize\n",
    "from pyannote.metrics.detection import DetectionPrecisionRecallFMeasure\n",
    "from pyannote.core import Segment\n",
    "from pyannote.audio.utils.preview import preview\n",
    "import torch\n",
    "\n",
    "def compute_model_fscore(model):\n",
    "\n",
    "    file = next(ami.test())    \n",
    "    \n",
    "    # use model to extract speech probability\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    inference = Inference(model, progress_hook='Processing...', device=device)\n",
    "    speech_prob = inference(file)\n",
    "    \n",
    "    # take hard decision using a 0.5 threshold\n",
    "    binarize = Binarize()\n",
    "    speech = binarize(speech_prob)\n",
    "    \n",
    "    # compute detection f-score\n",
    "    fscore = DetectionPrecisionRecallFMeasure()\n",
    "    fscore = fscore(\n",
    "        file['annotation'],     # this is the reference annotation\n",
    "        speech,                 # this is the hypothesized annotation\n",
    "        uem=file['annotated'])  # this is the part of the file that should be evaluated\n",
    "    \n",
    "    print(f'F-score = {100 * fscore:.1f}%')\n",
    "    \n",
    "    # preview results\n",
    "    # (comment if you don't care about visualization as it takes a relatively long amount of time to generate\n",
    "    second_minute = Segment(60, 120)\n",
    "    return preview(file, \n",
    "                   segment=second_minute, \n",
    "                   video_fps=5., \n",
    "                   reference=file['annotation'], \n",
    "                   probability=speech_prob, \n",
    "                   speech=speech.get_timeline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To serve as our baseline, we load a voice activity detection model pretrained on DIHARD III dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Model\n",
    "pretrained = Model.from_pretrained('hbredin/VoiceActivityDetection-PyanNet-DIHARD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `pretrained` model relies on the `PyanNet` architecture available in `pyannote.audio`, that combines (trainable) SincNet feature extraction, a few LSTM layers, a few linear layers and a final classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pretrained.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_model_fscore(pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train a voice activity detection model from scratch, using the AMI training set.\n",
    "\n",
    "To make sure we use the exact same architecture, we rely on `pretrained.hparams` that conveniently keeps track of the hyper-parameters used to instantiate the architecture of `pretrained` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio.models.segmentation import PyanNet\n",
    "from_scratch = PyanNet(task=vad, **pretrained.hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘€  Notice how we passed `vad` as the `task` argument of our `from_scratch` model.  \n",
    "This allows `pyannote.audio` to automagically register the right `classifier` and `activation` layers into the `PyanNet` model.\n",
    "\n",
    "> Look ma, no hands!\n",
    "\n",
    "This magic trick is possible because every task in `pyannote.audio` exposes its specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad.specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voice activity detection is a *binary classification* problem that is trained on *2s* audio chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=2)\n",
    "trainer.fit(from_scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_model_fscore(from_scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we do better (or at least faster) by fine-tuning the pretrained DIHARD model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned = Model.from_pretrained('hbredin/VoiceActivityDetection-PyanNet-DIHARD')\n",
    "fine_tuned.task = vad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this callback will freeze all layers, except \n",
    "# * 'classifier' which is always trained;\n",
    "# * 'linear' which the callback is asked to unfreeze as soon as epoch 0\n",
    "from pyannote.audio.core.callback import GraduallyUnfreeze\n",
    "callback = GraduallyUnfreeze({\"linear\": 0})\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=2, \n",
    "                     callbacks=[callback])\n",
    "trainer.fit(fine_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_model_fscore(fine_tuned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
